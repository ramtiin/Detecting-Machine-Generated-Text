{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nU99Sp21eVY1",
        "outputId": "52bac94b-d3e4-4d06-dd5d-37fe6f423b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.31.0-py3-none-any.whl (7.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.4/7.4 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Collecting huggingface-hub<1.0,>=0.14.1 (from transformers)\n",
            "  Downloading huggingface_hub-0.16.4-py3-none-any.whl (268 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m268.8/268.8 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m48.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n",
            "Installing collected packages: tokenizers, safetensors, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.16.4 safetensors-0.3.2 tokenizers-0.13.3 transformers-4.31.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "from collections import Counter, defaultdict\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk import sent_tokenize, word_tokenize\n",
        "import nltk\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "import xgboost as xgb\n",
        "from scipy.stats import zscore\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')"
      ],
      "metadata": {
        "id": "tFS3KCtvwJKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_and_preprocess_data(path):\n",
        "    # Load the dataframe\n",
        "    df = pd.read_csv(path)\n",
        "\n",
        "    # Split the data into human and GPT datasets\n",
        "    dfs = dict(tuple(df.groupby('label')))\n",
        "    Human_df = dfs[0].reset_index(drop=True)\n",
        "    GPT_df = dfs[1].reset_index(drop=True)\n",
        "\n",
        "    return Human_df, GPT_df\n",
        "\n",
        "def preprocess_dataframe(df):\n",
        "    # Make a copy to avoid warnings and unintended modifications\n",
        "    df_copy = df.copy()\n",
        "\n",
        "    # Rename the column\n",
        "    df_copy.rename(columns={\"answer\": \"text\"}, inplace=True)\n",
        "\n",
        "    # Drop unwanted columns\n",
        "    df_copy.drop(['id', 'question'], axis=1, inplace=True)\n",
        "\n",
        "    # Filter out rows with less than 10 tokens\n",
        "    df_copy = df_copy[df_copy['text'].str.split().str.len() >= 10]\n",
        "\n",
        "    return df_copy\n",
        "\n",
        "def split_train_val(df):\n",
        "    # Sample 7000 rows for validation data\n",
        "    df_val = df.sample(n=7000, random_state=42)\n",
        "\n",
        "    # Remove these rows from the original data to avoid overlap\n",
        "    df_train = df.drop(df_val.index).reset_index(drop=True)\n",
        "\n",
        "    return df_train, df_val\n",
        "\n",
        "# Load and preprocess data\n",
        "Human_df_train, GPT_df_train = load_and_preprocess_data('/content/drive/MyDrive/HC3 - filtered/en_train.csv')\n",
        "Human_df_test, GPT_df_test = load_and_preprocess_data('/content/drive/MyDrive/HC3 - filtered/en_test.csv')\n",
        "\n",
        "# Split training data into training and validation\n",
        "Human_df_train, Human_df_val = split_train_val(Human_df_train)\n",
        "GPT_df_train, GPT_df_val = split_train_val(GPT_df_train)\n",
        "\n",
        "# Preprocess dataframes\n",
        "Human_df_train = preprocess_dataframe(Human_df_train)\n",
        "GPT_df_train = preprocess_dataframe(GPT_df_train)\n",
        "Human_df_val = preprocess_dataframe(Human_df_val)\n",
        "GPT_df_val = preprocess_dataframe(GPT_df_val)\n",
        "Human_df_test = preprocess_dataframe(Human_df_test)\n",
        "GPT_df_test = preprocess_dataframe(GPT_df_test)"
      ],
      "metadata": {
        "id": "2WqB11UfTObF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Add source columns\n",
        "Human_df_train['source'] = 'train'\n",
        "GPT_df_train['source'] = 'train'\n",
        "\n",
        "Human_df_test['source'] = 'test'\n",
        "GPT_df_test['source'] = 'test'\n",
        "\n",
        "\n",
        "\n",
        "Human_df_val['source'] = 'val'\n",
        "GPT_df_val['source'] = 'val'\n",
        "\n",
        "# Concatenate all the datasets\n",
        "GPT_df = pd.concat([GPT_df_train, GPT_df_test, GPT_df_val], ignore_index=True)\n",
        "Human_df = pd.concat([Human_df_train, Human_df_test, Human_df_val], ignore_index=True)"
      ],
      "metadata": {
        "id": "Fa7i65Xybhky"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from tqdm import tqdm\n",
        "tqdm.pandas()\n",
        "\n",
        "class TextPreprocessor:\n",
        "    def __init__(self):\n",
        "        # Compile regex patterns for performance\n",
        "        self.space_before_punct = re.compile(r'\\s+([.,?!;:])')\n",
        "        self.space_after_punct = re.compile(r'([.,?!;:])\\s+')\n",
        "        self.contractions = re.compile(r\"(\\w) n\\'t\")\n",
        "        self.double_dashes = re.compile(r\"\\s*--\\s*\")\n",
        "        self.hyphens = re.compile(r\"\\s+-\\s+\")\n",
        "        self.single_quotes_start = re.compile(r\"(\\w)'(\\s)\")\n",
        "        self.single_quotes_end = re.compile(r\"(\\s)'(\\w)\")\n",
        "        self.mentions = re.compile(r'(@.*?)[\\s]')\n",
        "        self.links = re.compile(r'https?:\\/\\/[^\\s\\n\\r]+')\n",
        "        self.embedded_quotes = re.compile(r'\"\\s([^\"]+)\\s\"')\n",
        "        self.single_quotes_embedded = re.compile(r\"'\\s([^']+)\\s'\")\n",
        "        self.space_after_open_parenthesis = re.compile(r'\\(\\s')\n",
        "        self.space_before_close_parenthesis = re.compile(r'\\s\\)')\n",
        "        self.multi_spaces = re.compile(r'\\s+')\n",
        "\n",
        "    def remove_unicode(self,text):\n",
        "      return ''.join(char for char in text if ord(char) < 128)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        text = self.space_before_punct.sub(r'\\1', text)\n",
        "        text = self.space_after_punct.sub(r'\\1 ', text)\n",
        "        text = text.replace(\"\\\\'\", \"'\")\n",
        "        text = text.replace(\"\\n\", \" \").replace(\"\\\\\", \"\").replace('*', '')\n",
        "        text = self.remove_unicode(text)\n",
        "        text = self.contractions.sub(r\"\\1n't\", text)\n",
        "        text = self.double_dashes.sub(\"--\", text)\n",
        "        text = self.hyphens.sub(\"-\", text)\n",
        "        text = self.single_quotes_start.sub(r\"\\1'\", text)\n",
        "        text = self.single_quotes_end.sub(r\" '\\1\", text)\n",
        "        text = self.mentions.sub(' ', text)\n",
        "        text = self.links.sub(' ', text)\n",
        "        text = text.replace('#', ' ').replace(\"&amp;\", \"&\")\n",
        "        text = self.embedded_quotes.sub(r'\"\\1\"', text)\n",
        "        text = self.single_quotes_embedded.sub(r\"\\1\", text)\n",
        "        text = self.space_after_open_parenthesis.sub('(', text)\n",
        "        text = self.space_before_close_parenthesis.sub(')', text)\n",
        "        text = self.multi_spaces.sub(' ', text)\n",
        "        return text\n",
        "\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "GPT_df.text = GPT_df.text.progress_map(preprocessor.preprocess)\n",
        "Human_df.text = Human_df.text.progress_map(preprocessor.preprocess)"
      ],
      "metadata": {
        "id": "vbFffodAUxsi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c3f7d54-3c23-4776-8233-384f70a5af76"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 26752/26752 [00:07<00:00, 3712.53it/s]\n",
            "100%|██████████| 53857/53857 [00:10<00:00, 5063.87it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gpt_concatenated_string = ' '.join(GPT_df['text'])\n",
        "\n",
        "human_concatenated_string = ' '.join(Human_df['text'])"
      ],
      "metadata": {
        "id": "tidc9uv2CmVg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sVchE3RJJyjQ",
        "outputId": "576fda6f-9be8-4964-e209-4de70fb8f4d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "bat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install datasets\n",
        "!pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bJaqpbfgcWTj",
        "outputId": "5d402a36-ed3d-4ece-c69f-840aeb692749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.14.4-py3-none-any.whl (519 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m519.3/519.3 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.23.5)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.8,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.3/115.3 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.1)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.15-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.5)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.16.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (3.2.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.14.0->datasets) (4.7.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.7.22)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.14.4 dill-0.3.7 multiprocess-0.70.15 xxhash-3.3.0\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.31.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.16.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.3.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import Dataset\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, dataframe, tokenizer, max_length=500):\n",
        "        texts = dataframe.text.values.tolist()\n",
        "        #texts = [self._preprocess(text) for text in texts]\n",
        "        self._print_random_samples(texts)\n",
        "\n",
        "        self.texts = tokenizer(texts,\n",
        "                              add_special_tokens=True,\n",
        "                              max_length=max_length,\n",
        "                              padding='max_length',\n",
        "                              return_token_type_ids=True,  # Include this only if necessary for your task/model\n",
        "                              truncation=True,\n",
        "                              return_tensors=\"pt\")\n",
        "\n",
        "        if 'label' in dataframe:\n",
        "            self.labels = dataframe.label.values.tolist()\n",
        "\n",
        "    def _print_random_samples(self, texts):\n",
        "        random_entries = np.random.choice(len(texts), 5, replace=False)\n",
        "        for i in random_entries:\n",
        "            print(f\"Entry {i}: {texts[i]}\")\n",
        "        print()\n",
        "\n",
        "    #def _preprocess(self, text):\n",
        "     #   text = preprocessor(text)  # Assuming preprocessor function is defined elsewhere\n",
        "      #  return text\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts['input_ids'])\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = {'input_ids': self.texts['input_ids'][idx],\n",
        "                'attention_mask': self.texts['attention_mask'][idx]}\n",
        "        label = -1\n",
        "        if hasattr(self, 'labels'):\n",
        "            label = self.labels[idx]\n",
        "        return text, label"
      ],
      "metadata": {
        "id": "MpyjECz9uYr-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "\n",
        "class Classifier(nn.Module):\n",
        "    def __init__(self, base_model):\n",
        "        super(Classifier, self).__init__()\n",
        "\n",
        "        self.bert = base_model\n",
        "        self.fc1 = nn.Linear(768, 32)\n",
        "        self.fc2 = nn.Linear(32, 1)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        bert_out = self.bert(input_ids=input_ids,\n",
        "                             attention_mask=attention_mask)[0][:, 0]\n",
        "        x = self.fc1(bert_out)\n",
        "        x = self.relu(x)\n",
        "\n",
        "        x = self.fc2(x)\n",
        "        x = self.sigmoid(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "s-UWg9Awtxud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "def train(model, train_dataloader, val_dataloader, learning_rate, epochs):\n",
        "    best_val_loss = float('inf')\n",
        "    early_stopping_threshold_count = 0\n",
        "\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    criterion = nn.BCELoss()\n",
        "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "    model = model.to(device)\n",
        "    criterion = criterion.to(device)\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        total_acc_train = 0\n",
        "        total_loss_train = 0\n",
        "\n",
        "        model.train()\n",
        "\n",
        "        for train_input, train_label in tqdm(train_dataloader):\n",
        "            attention_mask = train_input['attention_mask'].to(device)\n",
        "            input_ids = train_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "            train_label = train_label.to(device)\n",
        "\n",
        "            output = model(input_ids, attention_mask)\n",
        "\n",
        "            loss = criterion(output, train_label.float().unsqueeze(1))\n",
        "\n",
        "            total_loss_train += loss.item()\n",
        "\n",
        "            acc = ((output >= 0.5).int() == train_label.unsqueeze(1)).sum().item()\n",
        "            total_acc_train += acc\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            total_acc_val = 0\n",
        "            total_loss_val = 0\n",
        "\n",
        "            model.eval()\n",
        "\n",
        "            for val_input, val_label in tqdm(val_dataloader):\n",
        "                attention_mask = val_input['attention_mask'].to(device)\n",
        "                input_ids = val_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "                val_label = val_label.to(device)\n",
        "\n",
        "                output = model(input_ids, attention_mask)\n",
        "\n",
        "                loss = criterion(output, val_label.float().unsqueeze(1))\n",
        "\n",
        "                total_loss_val += loss.item()\n",
        "\n",
        "                acc = ((output >= 0.5).int() == val_label.unsqueeze(1)).sum().item()\n",
        "                total_acc_val += acc\n",
        "\n",
        "            print(f'Epochs: {epoch + 1} '\n",
        "                  f'| Train Loss: {total_loss_train / len(train_dataloader): .3f} '\n",
        "                  f'| Train Accuracy: {total_acc_train / (len(train_dataloader.dataset)): .3f} '\n",
        "                  f'| Val Loss: {total_loss_val / len(val_dataloader): .3f} '\n",
        "                  f'| Val Accuracy: {total_acc_val / len(val_dataloader.dataset): .3f}')\n",
        "\n",
        "            if best_val_loss > total_loss_val:\n",
        "                best_val_loss = total_loss_val\n",
        "                torch.save(model, f\"best_model.pt\")\n",
        "                print(\"Saved model\")\n",
        "                early_stopping_threshold_count = 0\n",
        "            else:\n",
        "                early_stopping_threshold_count += 1\n",
        "\n",
        "            if early_stopping_threshold_count >= 1:\n",
        "                print(\"Early stopping\")\n",
        "                break"
      ],
      "metadata": {
        "id": "xH-LcmjZuB0z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import gc\n",
        "\n",
        "# Empty the cache\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "# Manually collect garbage\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4D0scvYys7cR",
        "outputId": "9bc5f082-b560-408a-b3f5-8fec98bcd383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Set random seeds\n",
        "torch.manual_seed(0)\n",
        "np.random.seed(0)\n",
        "\n",
        "# BERT model definition\n",
        "BERT_MODEL = \"roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(BERT_MODEL)\n",
        "base_model = AutoModel.from_pretrained(BERT_MODEL)\n",
        "\n",
        "# Concatenate dataframes\n",
        "df = pd.concat([Human_df, GPT_df])\n",
        "\n",
        "# Split dataframes into train and test\n",
        "df_train = df[df['source'] == 'train']\n",
        "df_val = df[df['source'] == 'val']\n",
        "\n",
        "# Initialize data loaders\n",
        "train_dataloader = DataLoader(TextDataset(df_train, tokenizer), batch_size=16, shuffle=True, num_workers=0)\n",
        "val_dataloader = DataLoader(TextDataset(df_val, tokenizer), batch_size=16, num_workers=0)\n",
        "\n",
        "# Initialize classifier model\n",
        "model = Classifier(base_model)  # Assuming Classifier class is defined elsewhere"
      ],
      "metadata": {
        "id": "zYxU7onAKu8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1794bcc7-f892-440d-a36a-5d80875f7b24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry 32129: Idaho is famous for its potatoes because the state has ideal growing conditions for potatoes. The soil in Idaho is rich and fertile, and the state's climate is perfect for growing potatoes. Idaho also has a long history of potato farming, which has helped to make it well-known as a source of high-quality potatoes. In addition, the state has done a good job of promoting its potatoes through marketing and branding efforts, which has helped to increase awareness of Idaho potatoes around the country and the world. So, Idaho is famous for its potatoes because it has the right conditions for growing them, and because it has worked hard to promote its potatoes to the public.\n",
            "Entry 9020: They e designed to hold up our abdomens, and that about it. We ' e supposed to use out legs and arms for strength\n",
            "Entry 35572: If you were to jump into a pool of jello, you would most likely sink to the bottom because jello is a type of soft, solid food that is made from gelatin and often has fruit pieces or other ingredients mixed in. It is not a liquid, so it would not be able to support your body weight in the same way that water can. When you jump into a pool of water, your body is able to float because water is denser than air, so it pushes up against your body and helps to support it. Jello is not dense enough to do this, so you would sink to the bottom. You might feel like you are stuck in the jello because it is thick and gooey, and it might be hard to move around. It would not be a very comfortable or enjoyable experience!\n",
            "Entry 27001: America has been country of plenty since the Great Depression. Starting in the 40 ' after WWII, America has been in a trend of more and more. From food being readily available to houses being bigger to even cars being bigger. Coffee cups are just following the trend that \"more is always better.\" To run out of something is to fail to have enough, and failure is terribly looked down upon in every sense.\n",
            "Entry 4672: As the water crystallizes into ice, it expands. Try sticking a water bottle in the freezer and you l see this happen when the bottle pops open (or the bottom gets pushed out. There not more water getting added, it simply expanding as the crystals form. Well, you have less material in the same amount of space. That means the density of the ice is lower than the density of the water. That why it floats!\n",
            "\n",
            "Entry 470: Ex-Hacker who got caught by the FBI here: Originally when I began writing viruses I started with a simple one that allowed downloaded cached passwords out of douchery. Eventually I started evolving the code to where it was installed on a USB and connecting it to a PC for three seconds would download all cached passwords, network logins, internet history, and various other small informative files. This is not what I was busted for however. After a year of cracking and hacking I developed a RAT that I based around the same USB premise. I had a 2 GB USB about the size of a Logitech unifying receiver that when connected installed the RAT in five seconds flat. I updated the code every day and was able to avoid virus detection by attaching it as a internet explorer toolbar that did nothing. Fast forward six months and I ' e spread it locally and online from California to Lithuania. I had about 76,000 connections and would be paid to DDoS sites. Fast forward four more months and I had reached 100,000 connections and set up a system to allow people to rent my infected users through the black market on TOR. What they did was their business. Everything went smooth for another five months and had reached ~135,000 when I got the knock on the door. Two federal agents wishing to investigate a possible disturbance located at my house. I was under sixteen when this had happened so I was very scared and admitted everything. I explained everything I knew, have them everything, and didn't here anything for a month. Eventually they were at the door again and I fully expected the worst and expected being sent to juvenile detention or something. They ended up restricting my PC access for two years and told me to be more productive with my time while also handing me four computer science books that I read like hell. Now I work in Cyber Forensics and get coffee every day with the two agents as we all work together now. They love to reminisce on how terrified I look. TL;DR I did illegal hacking, got caught, made a career out of it\n",
            "Entry 11816: Limit orders are generally considered to be safe because they allow you to specify the maximum price you are willing to pay (for a buy order) or the minimum price you are willing to accept (for a sell order). This means that you will only execute the trade if the market price is at or better than your specified limit price.However, it's important to note that there are no guarantees when it comes to trading and the use of limit orders does not guarantee that your trade will be executed. Market conditions can change rapidly, and there is always the possibility that your limit order may not be filled if the market price never reaches your specified limit price.Additionally, it's important to be aware of the potential risks associated with trading, including the risk of loss. It's always a good idea to thoroughly research and understand the risks before making any trades.\n",
            "Entry 2553: > It not like you e changing the outcome. Well, you are changing the outcome. When you e aware of all the probabilities and you e able to make the most statistically sensible decision, you have an advantage over the players who aren't aware of all the odds. You might be aware of this, but casinos generally don't like it when the odds shift towards the players instead of the house. It kind of like the casino version of counting cards – kicking out the people who count cards.\n",
            "Entry 4720: I am an offshore / subsea engineer. I work with Remotely Operated Vehicle (ROV) pilots who have probably seen more of the seabed than any other group of people on the planet. I e seen a fair bit of it myself. It very dull. The bottom of the sea looks like a desert. There ' nothing there, just sand a rocks. Only in the shallower areas of the world where the water depth is less than 100 m do you actually see real fish. There few fish that live down at 300 m, and even fewer that live deeper than that. So you need to define \"explored\". We have satellite and sonar surveys of the entire ocean, and we know how deep it is in these locations and usually what kind of soil or or rock is down there. You could just as well say that not all of the Sahara Desert is explored-There are bits of it that no one has probably ever seen, but they look just like all the other bits. A.\n",
            "Entry 13146: A panic attack is a sudden and intense feeling of fear or anxiety that can be accompanied by physical symptoms such as a racing heart, shortness of breath, dizziness, and chest pain. These symptoms can be very frightening and can make someone feel like they are in danger, even if there is no real danger present. If you come home and see your housemate suffering from a panic attack, there are some things you can do to help: 1. Stay calm and try to stay with them. Panic attacks can be scary, and your housemate may feel very alone and overwhelmed. By staying with them, you can provide support and comfort. 2. Encourage them to take slow, deep breaths. This can help to calm their body and mind. 3. Talk to them in a soothing and reassuring voice. Let them know that they are not alone and that you are there to help. 4. Help them find a comfortable and safe place to sit or lie down. 5. Offer them a glass of water or a soothing drink. 6. If the panic attack continues for a long time or if your housemate is in severe distress, consider calling for medical help. Remember, panic attacks are not dangerous, and they will usually pass on their own after a few minutes. It's important to reassure your housemate and let them know that they are safe and that the panic attack will eventually end.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "learning_rate = 1e-5\n",
        "epochs = 1\n",
        "\n",
        "# Train model\n",
        "train(model, train_dataloader, val_dataloader, learning_rate, epochs)  # Assuming train function is defined elsewhere"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X6Fi3tFnf1xX",
        "outputId": "d88ebd4f-23f8-4de4-befe-e4584c840fc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2669/2669 [19:40<00:00,  2.26it/s]\n",
            "100%|██████████| 860/860 [02:04<00:00,  6.92it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epochs: 1 | Train Loss:  0.035 | Train Accuracy:  0.988 | Val Loss:  0.035 | Val Accuracy:  0.990\n",
            "Saved model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_text_predictions(model, loader):\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
        "\n",
        "    model = model.to(device)\n",
        "\n",
        "\n",
        "    results_predictions = []\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        for data_input, _ in tqdm(loader):\n",
        "            attention_mask = data_input['attention_mask'].to(device)\n",
        "            input_ids = data_input['input_ids'].squeeze(1).to(device)\n",
        "\n",
        "\n",
        "            output = model(input_ids, attention_mask)\n",
        "\n",
        "            output = (output > 0.5).int()\n",
        "            results_predictions.append(output)\n",
        "\n",
        "    return torch.cat(results_predictions).cpu().detach().numpy()"
      ],
      "metadata": {
        "id": "bmtZbtBzqmSf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.save(model, \"/content/drive/MyDrive/Project/roberta_gpt3_filtered.pt\")\n"
      ],
      "metadata": {
        "id": "9HviufbF83rw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model = torch.load(\"/content/drive/MyDrive/Project/roberta_gpt3_unfiltered.pt\",map_location=torch.device('cuda') )\n",
        "\n",
        "#test_dataloader = DataLoader(Dataset(web_df.iloc[0:10], tokenizer),\n",
        "#\tbatch_size=8, shuffle=False, num_workers=0)"
      ],
      "metadata": {
        "id": "yGhQ19Z6qpLq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "df_test= df[df['source'] == 'test']\n",
        "test_dataset = TextDataset(df_test, tokenizer)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
        "\n",
        "# 1. Extract true labels\n",
        "true_labels = []\n",
        "for _, labels in test_dataloader:\n",
        "    if isinstance(labels, torch.Tensor):\n",
        "        labels = labels.numpy()\n",
        "    true_labels.extend(labels)\n",
        "\n",
        "\n",
        "predictions = get_text_predictions(model, test_dataloader)\n",
        "# Flatten predictions if they are batched\n",
        "predictions = [item for sublist in predictions for item in sublist] if isinstance(predictions[0], (list, np.ndarray)) else predictions\n",
        "\n",
        "# 2. Generate classification report\n",
        "report = classification_report(true_labels, predictions)\n",
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBel2V8LtYJG",
        "outputId": "c763042d-8209-4300-ff77-129fc8660010"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Entry 13768: lay down, put a blanket on you, now another blanket, now another one. Eventually you l be like \"shit this is heavy\". That because all of those objects are pushing down on you. Water does the same thing, air does too but we ' e used to it.\n",
            "Entry 16437: Diesel fuel and gasoline are two different types of fuel that are used in vehicles. Diesel fuel is a type of fuel that is made from crude oil and is used mainly in diesel engines. Gasoline is also made from crude oil, but it is a different type of fuel that is used mainly in gasoline engines. The reason why gas stations have \"caution diesel\"signs on the pumps is because diesel fuel is not suitable for use in gasoline engines. If someone accidentally puts diesel fuel into a gasoline engine, it can cause serious damage to the engine. The\"caution diesel\" signs are there to remind people to make sure they are putting the right type of fuel into their vehicle. Gasoline engines and diesel engines are designed differently, and they require different types of fuel to operate properly. Diesel fuel has a higher energy content than gasoline, so it is more efficient in some ways. However, gasoline engines are not designed to burn diesel fuel, and using diesel fuel in a gasoline engine can cause problems. That's why it's important to use the right type of fuel for your vehicle.\n",
            "Entry 17301: People who are born deaf do not think differently from people who can hear. They are able to think and communicate just like everyone else. They might use sign language to communicate, which is a way of using their hands and body to express thoughts and feelings. They can also use written language, like reading and writing, to communicate and express themselves. Deaf people can also understand and experience emotions just like hearing people. They might be able to express their emotions through their body language, facial expressions, and through the way they communicate with others. Just because a person is born deaf does not mean they are any less able to think or understand things.\n",
            "Entry 4663: They can. From the attacker point of view, infecting a router has some advantages. It likely to go undetected, so the malware can be useful for longer. It ' also likely that if the malware is discovered in general, routers won't be quickly fixed to stop it. There the potential to spy on or interfere with all internet access from the network. It could be a springboard ' to infect devices on the network from. On the other hand it also has some drawbacks. There are numerous router models and probably the piece of malware only works on a few, so there aren't as many targets. The router can't snoop on device keystrokes and probably can't snoop on secure website connections, so the malware probably can not steal login details. The router is basically a very slow computer (except for being fast at networking stuff) so anything that needs a lot of performance, such as password cracking or bitcoin mining, won't work well. And a router offers a crude and limited system to program anything, including malware, for when compared to the features Windows offers.\n",
            "Entry 11835: The operating system maintains something called a File Allocation Table (FAT) on the USB device. This is an index of all the files on the drive; when you open the drive in Windows, it uses the index to quickly look up the files instead of having to physically search the entire drive to find them all. When you format the USB drive (or any drive), all you are doing is deleting the FAT and replacing it with a blank one. This operation takes just a few seconds, because it ' just one step and a new FAT isn't very large. The data for the old files is still there, but the new FAT doesn't have a record of them, so the OS doesn't know they are there. When you delete a file, the system is merely going into the FAT and marking that file as \"deleted\". It isn't actually removing the data. If you ' e deleting hundreds of files at once, then it has to locate the FAT entry for each file and mark it as \"deleted\", hundreds of times. This takes a lot longer than simply writing a new FAT, as happens when you format. TL;DR: The FAT is an index of all of the files on the disk. Deleting all the files means editing each FAT entry to \"deleted\", whereas formatting the disk means writing a brand new, blank FAT. If there are lots of files, the latter option is faster.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 3020/3020 [03:39<00:00, 13.79it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.98      0.99     16148\n",
            "           1       0.96      1.00      0.98      8012\n",
            "\n",
            "    accuracy                           0.99     24160\n",
            "   macro avg       0.98      0.99      0.98     24160\n",
            "weighted avg       0.99      0.99      0.99     24160\n",
            "\n"
          ]
        }
      ]
    }
  ]
}